{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StratifiedKFold(_BaseKFold):\n",
    "    \"\"\"Stratified K-Folds cross-validator\n",
    "\n",
    "    Provides train/test indices to split data in train/test sets.\n",
    "\n",
    "    This cross-validation object is a variation of KFold that returns\n",
    "    stratified folds. The folds are made by preserving the percentage of\n",
    "    samples for each class.\n",
    "\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=3\n",
    "        Number of folds. Must be at least 2.\n",
    "\n",
    "        .. versionchanged:: 0.20\n",
    "            ``n_splits`` default value will change from 3 to 5 in v0.22.\n",
    "\n",
    "    shuffle : boolean, optional\n",
    "        Whether to shuffle each stratification of the data before splitting\n",
    "        into batches.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional, default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Used when ``shuffle`` == True.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import StratifiedKFold\n",
    "    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "    >>> y = np.array([0, 0, 1, 1])\n",
    "    >>> skf = StratifiedKFold(n_splits=2)\n",
    "    >>> skf.get_n_splits(X, y)\n",
    "    2\n",
    "    >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
    "    >>> for train_index, test_index in skf.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [1 3] TEST: [0 2]\n",
    "    TRAIN: [0 2] TEST: [1 3]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be different in each fold, with a difference of at\n",
    "    most ``n_classes``.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits='warn', shuffle=False, random_state=None):\n",
    "        if n_splits == 'warn':\n",
    "            warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
    "            n_splits = 3\n",
    "        super().__init__(n_splits, shuffle, random_state)\n",
    "\n",
    "    def _make_test_folds(self, X, y=None):\n",
    "        rng = self.random_state\n",
    "        y = np.asarray(y)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "        allowed_target_types = ('binary', 'multiclass')\n",
    "        if type_of_target_y not in allowed_target_types:\n",
    "            raise ValueError(\n",
    "                'Supported target types are: {}. Got {!r} instead.'.format(\n",
    "                    allowed_target_types, type_of_target_y))\n",
    "\n",
    "        y = column_or_1d(y)\n",
    "        n_samples = y.shape[0]\n",
    "        unique_y, y_inversed = np.unique(y, return_inverse=True)\n",
    "        y_counts = np.bincount(y_inversed)\n",
    "        min_groups = np.min(y_counts)\n",
    "        if np.all(self.n_splits > y_counts):\n",
    "            raise ValueError(\"n_splits=%d cannot be greater than the\"\n",
    "                             \" number of members in each class.\"\n",
    "                             % (self.n_splits))\n",
    "        if self.n_splits > min_groups:\n",
    "            warnings.warn((\"The least populated class in y has only %d\"\n",
    "                           \" members, which is too few. The minimum\"\n",
    "                           \" number of members in any class cannot\"\n",
    "                           \" be less than n_splits=%d.\"\n",
    "                           % (min_groups, self.n_splits)), Warning)\n",
    "\n",
    "        # pre-assign each sample to a test fold index using individual KFold\n",
    "        # splitting strategies for each class so as to respect the balance of\n",
    "        # classes\n",
    "        # NOTE: Passing the data corresponding to ith class say X[y==class_i]\n",
    "        # will break when the data is not 100% stratifiable for all classes.\n",
    "        # So we pass np.zeroes(max(c, n_splits)) as data to the KFold\n",
    "        per_cls_cvs = [\n",
    "            KFold(self.n_splits, shuffle=self.shuffle,\n",
    "                  random_state=rng).split(np.zeros(max(count, self.n_splits)))\n",
    "            for count in y_counts]\n",
    "\n",
    "        test_folds = np.zeros(n_samples, dtype=np.int)\n",
    "        for test_fold_indices, per_cls_splits in enumerate(zip(*per_cls_cvs)):\n",
    "            for cls, (_, test_split) in zip(unique_y, per_cls_splits):\n",
    "                cls_test_folds = test_folds[y == cls]\n",
    "                # the test split can be too big because we used\n",
    "                # KFold(...).split(X[:max(c, n_splits)]) when data is not 100%\n",
    "                # stratifiable for all the classes\n",
    "                # (we use a warning instead of raising an exception)\n",
    "                # If this is the case, let's trim it:\n",
    "                test_split = test_split[test_split < len(cls_test_folds)]\n",
    "                cls_test_folds[test_split] = test_fold_indices\n",
    "                test_folds[y == cls] = cls_test_folds\n",
    "\n",
    "        return test_folds\n",
    "\n",
    "    def _iter_test_masks(self, X, y=None, groups=None):\n",
    "        test_folds = self._make_test_folds(X, y)\n",
    "        for i in range(self.n_splits):\n",
    "            yield test_folds == i\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target variable for supervised learning problems.\n",
    "            Stratification is done based on the y labels.\n",
    "\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super().split(X, y, groups)\n",
    "\n",
    "\n",
    "class TimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator\n",
    "\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals, in train/test sets.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=3\n",
    "        Number of splits. Must be at least 2.\n",
    "\n",
    "        .. versionchanged:: 0.20\n",
    "            ``n_splits`` default value will change from 3 to 5 in v0.22.\n",
    "\n",
    "    max_train_size : int, optional\n",
    "        Maximum size for a single training set.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import TimeSeriesSplit\n",
    "    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "    >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    "    >>> tscv = TimeSeriesSplit(n_splits=5)\n",
    "    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    TimeSeriesSplit(max_train_size=None, n_splits=5)\n",
    "    >>> for train_index, test_index in tscv.split(X):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0] TEST: [1]\n",
    "    TRAIN: [0 1] TEST: [2]\n",
    "    TRAIN: [0 1 2] TEST: [3]\n",
    "    TRAIN: [0 1 2 3] TEST: [4]\n",
    "    TRAIN: [0 1 2 3 4] TEST: [5]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The training set has size ``i * n_samples // (n_splits + 1)\n",
    "    + n_samples % (n_splits + 1)`` in the ``i``th split,\n",
    "    with a test set of size ``n_samples//(n_splits + 1)``,\n",
    "    where ``n_samples`` is the number of samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits='warn', max_train_size=None):\n",
    "        if n_splits == 'warn':\n",
    "            warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
    "            n_splits = 3\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "\n",
    "        groups : array-like, with shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        if n_folds > n_samples:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds ={0} greater\"\n",
    "                 \" than the number of samples: {1}.\").format(n_folds,\n",
    "                                                             n_samples))\n",
    "        indices = np.arange(n_samples)\n",
    "        test_size = (n_samples // n_folds)\n",
    "        test_starts = range(test_size + n_samples % n_folds,\n",
    "                            n_samples, test_size)\n",
    "        for test_start in test_starts:\n",
    "            if self.max_train_size and self.max_train_size < test_start:\n",
    "                yield (indices[test_start - self.max_train_size:test_start],\n",
    "                       indices[test_start:test_start + test_size])\n",
    "            else:\n",
    "                yield (indices[:test_start],\n",
    "                       indices[test_start:test_start + test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Restricted Boltzmann Machine\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Yann N. Dauphin <dauphiya@iro.umontreal.ca>\n",
    "#          Vlad Niculae\n",
    "#          Gabriel Synnaeve\n",
    "#          Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.special import expit  # logistic function\n",
    "\n",
    "from ..base import BaseEstimator\n",
    "from ..base import TransformerMixin\n",
    "from ..utils import check_array\n",
    "from ..utils import check_random_state\n",
    "from ..utils import gen_even_slices\n",
    "from ..utils.extmath import safe_sparse_dot\n",
    "from ..utils.extmath import log_logistic\n",
    "from ..utils.validation import check_is_fitted\n",
    "\n",
    "\n",
    "class BernoulliRBM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Bernoulli Restricted Boltzmann Machine (RBM).\n",
    "\n",
    "    A Restricted Boltzmann Machine with binary visible units and\n",
    "    binary hidden units. Parameters are estimated using Stochastic Maximum\n",
    "    Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n",
    "    [2].\n",
    "\n",
    "    The time complexity of this implementation is ``O(d ** 2)`` assuming\n",
    "    d ~ n_features ~ n_components.\n",
    "\n",
    "    Read more in the :ref:`User Guide <rbm>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, optional\n",
    "        Number of binary hidden units.\n",
    "\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for weight updates. It is *highly* recommended\n",
    "        to tune this hyper-parameter. Reasonable values are in the\n",
    "        10**[0., -3.] range.\n",
    "\n",
    "    batch_size : int, optional\n",
    "        Number of examples per minibatch.\n",
    "\n",
    "    n_iter : int, optional\n",
    "        Number of iterations/sweeps over the training dataset to perform\n",
    "        during training.\n",
    "\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode.\n",
    "\n",
    "    random_state : integer or RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    intercept_hidden_ : array-like, shape (n_components,)\n",
    "        Biases of the hidden units.\n",
    "\n",
    "    intercept_visible_ : array-like, shape (n_features,)\n",
    "        Biases of the visible units.\n",
    "\n",
    "    components_ : array-like, shape (n_components, n_features)\n",
    "        Weight matrix, where n_features in the number of\n",
    "        visible units and n_components is the number of hidden units.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.neural_network import BernoulliRBM\n",
    "    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "    >>> model = BernoulliRBM(n_components=2)\n",
    "    >>> model.fit(X)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
    "           random_state=None, verbose=0)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\n",
    "        deep belief nets. Neural Computation 18, pp 1527-1554.\n",
    "        https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n",
    "\n",
    "    [2] Tieleman, T. Training Restricted Boltzmann Machines using\n",
    "        Approximations to the Likelihood Gradient. International Conference\n",
    "        on Machine Learning (ICML) 2008\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=256, learning_rate=0.1, batch_size=10,\n",
    "                 n_iter=10, verbose=0, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            The data to be transformed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        h : array, shape (n_samples, n_components)\n",
    "            Latent representations of the data.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"components_\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        return self._mean_hiddens(X)\n",
    "\n",
    "    def _mean_hiddens(self, v):\n",
    "        \"\"\"Computes the probabilities P(h=1|v).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        \"\"\"\n",
    "        p = safe_sparse_dot(v, self.components_.T)\n",
    "        p += self.intercept_hidden_\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def _sample_hiddens(self, v, rng):\n",
    "        \"\"\"Sample from the distribution P(h|v).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer to sample from.\n",
    "\n",
    "        rng : RandomState\n",
    "            Random number generator to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer.\n",
    "        \"\"\"\n",
    "        p = self._mean_hiddens(v)\n",
    "        return (rng.random_sample(size=p.shape) < p)\n",
    "\n",
    "    def _sample_visibles(self, h, rng):\n",
    "        \"\"\"Sample from the distribution P(v|h).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer to sample from.\n",
    "\n",
    "        rng : RandomState\n",
    "            Random number generator to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        \"\"\"\n",
    "        p = np.dot(h, self.components_)\n",
    "        p += self.intercept_visible_\n",
    "        expit(p, out=p)\n",
    "        return (rng.random_sample(size=p.shape) < p)\n",
    "\n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the free energy F(v) = - log sum_h exp(-E(v,h)).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        return (- safe_sparse_dot(v, self.intercept_visible_)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.components_.T)\n",
    "                               + self.intercept_hidden_).sum(axis=1))\n",
    "\n",
    "    def gibbs(self, v):\n",
    "        \"\"\"Perform one Gibbs sampling step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer to start from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        v_new : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer after one Gibbs step.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"components_\")\n",
    "        if not hasattr(self, \"random_state_\"):\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "        h_ = self._sample_hiddens(v, self.random_state_)\n",
    "        v_ = self._sample_visibles(h_, self.random_state_)\n",
    "\n",
    "        return v_\n",
    "\n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X which should contain a partial\n",
    "        segment of the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : BernoulliRBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        if not hasattr(self, 'random_state_'):\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "        if not hasattr(self, 'components_'):\n",
    "            self.components_ = np.asarray(\n",
    "                self.random_state_.normal(\n",
    "                    0,\n",
    "                    0.01,\n",
    "                    (self.n_components, X.shape[1])\n",
    "                ),\n",
    "                order='F')\n",
    "        if not hasattr(self, 'intercept_hidden_'):\n",
    "            self.intercept_hidden_ = np.zeros(self.n_components, )\n",
    "        if not hasattr(self, 'intercept_visible_'):\n",
    "            self.intercept_visible_ = np.zeros(X.shape[1], )\n",
    "        if not hasattr(self, 'h_samples_'):\n",
    "            self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        self._fit(X, self.random_state_)\n",
    "\n",
    "    def _fit(self, v_pos, rng):\n",
    "        \"\"\"Inner fit for one mini-batch.\n",
    "\n",
    "        Adjust the parameters to maximize the likelihood of v using\n",
    "        Stochastic Maximum Likelihood (SML).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v_pos : array-like, shape (n_samples, n_features)\n",
    "            The data to use for training.\n",
    "\n",
    "        rng : RandomState\n",
    "            Random number generator to use for sampling.\n",
    "        \"\"\"\n",
    "        h_pos = self._mean_hiddens(v_pos)\n",
    "        v_neg = self._sample_visibles(self.h_samples_, rng)\n",
    "        h_neg = self._mean_hiddens(v_neg)\n",
    "\n",
    "        lr = float(self.learning_rate) / v_pos.shape[0]\n",
    "        update = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T\n",
    "        update -= np.dot(h_neg.T, v_neg)\n",
    "        self.components_ += lr * update\n",
    "        self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "        self.intercept_visible_ += lr * (np.asarray(\n",
    "                                         v_pos.sum(axis=0)).squeeze() -\n",
    "                                         v_neg.sum(axis=0))\n",
    "\n",
    "        h_neg[rng.uniform(size=h_neg.shape) < h_neg] = 1.0  # sample binomial\n",
    "        self.h_samples_ = np.floor(h_neg, h_neg)\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes a quantity called the\n",
    "        free energy on X, then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"components_\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        # Randomly corrupt one feature in each sample in v.\n",
    "        ind = (np.arange(v.shape[0]),\n",
    "               rng.randint(0, v.shape[1], v.shape[0]))\n",
    "        if sp.issparse(v):\n",
    "            data = -2 * v[ind] + 1\n",
    "            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n",
    "        else:\n",
    "            v_ = v.copy()\n",
    "            v_[ind] = 1 - v_[ind]\n",
    "\n",
    "        fe = self._free_energy(v)\n",
    "        fe_ = self._free_energy(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : BernoulliRBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        n_samples = X.shape[0]\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        self.components_ = np.asarray(\n",
    "            rng.normal(0, 0.01, (self.n_components, X.shape[1])),\n",
    "            order='F')\n",
    "        self.intercept_hidden_ = np.zeros(self.n_components, )\n",
    "        self.intercept_visible_ = np.zeros(X.shape[1], )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
    "                                            n_batches, n_samples))\n",
    "        verbose = self.verbose\n",
    "        begin = time.time()\n",
    "        for iteration in range(1, self.n_iter + 1):\n",
    "            for batch_slice in batch_slices:\n",
    "                self._fit(X[batch_slice], rng)\n",
    "\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\n",
    "                      \" time = %.2fs\"\n",
    "                      % (type(self).__name__, iteration,\n",
    "                         self.score_samples(X).mean(), end - begin))\n",
    "                begin = end\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================================\n",
    "Restricted Boltzmann Machine features for digit classification\n",
    "==============================================================\n",
    "\n",
    "For greyscale image data where pixel values can be interpreted as degrees of\n",
    "blackness on a white background, like handwritten digit recognition, the\n",
    "Bernoulli Restricted Boltzmann machine model (:class:`BernoulliRBM\n",
    "<sklearn.neural_network.BernoulliRBM>`) can perform effective non-linear\n",
    "feature extraction.\n",
    "\n",
    "In order to learn good latent representations from a small dataset, we\n",
    "artificially generate more labeled data by perturbing the training data with\n",
    "linear shifts of 1 pixel in each direction.\n",
    "\n",
    "This example shows how to build a classification pipeline with a BernoulliRBM\n",
    "feature extractor and a :class:`LogisticRegression\n",
    "<sklearn.linear_model.LogisticRegression>` classifier. The hyperparameters\n",
    "of the entire model (learning rate, hidden layer size, regularization)\n",
    "were optimized by grid search, but the search is not reproduced here because\n",
    "of runtime constraints.\n",
    "\n",
    "Logistic regression on raw pixel values is presented for comparison. The\n",
    "example shows that the features extracted by the BernoulliRBM help improve the\n",
    "classification accuracy.\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "# Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve\n",
    "# License: BSD\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Setting up\n",
    "\n",
    "def nudge_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    This produces a dataset 5 times bigger than the original one,\n",
    "    by moving the 8x8 images in X around by 1px to left, right, down, up\n",
    "    \"\"\"\n",
    "    direction_vectors = [\n",
    "        [[0, 1, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [1, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 1],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 1, 0]]]\n",
    "\n",
    "    def shift(x, w):\n",
    "        return convolve(x.reshape((8, 8)), mode='constant', weights=w).ravel()\n",
    "\n",
    "    X = np.concatenate([X] +\n",
    "                       [np.apply_along_axis(shift, 1, X, vector)\n",
    "                        for vector in direction_vectors])\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Load Data\n",
    "digits = datasets.load_digits()\n",
    "X = np.asarray(digits.data, 'float32')\n",
    "X, Y = nudge_dataset(X, digits.target)\n",
    "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Models we will use\n",
    "logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=10000,\n",
    "                                           multi_class='multinomial')\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "rbm_features_classifier = Pipeline(\n",
    "    steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "\n",
    "# #############################################################################\n",
    "# Training\n",
    "\n",
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 20\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 100\n",
    "logistic.C = 6000\n",
    "\n",
    "# Training RBM-Logistic Pipeline\n",
    "rbm_features_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Training the Logistic regression classifier directly on the pixel\n",
    "raw_pixel_classifier = clone(logistic)\n",
    "raw_pixel_classifier.C = 100.\n",
    "raw_pixel_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# #############################################################################\n",
    "# Evaluation\n",
    "\n",
    "Y_pred = rbm_features_classifier.predict(X_test)\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(Y_test, Y_pred)))\n",
    "\n",
    "Y_pred = raw_pixel_classifier.predict(X_test)\n",
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(Y_test, Y_pred)))\n",
    "\n",
    "# #############################################################################\n",
    "# Plotting\n",
    "\n",
    "plt.figure(figsize=(4.2, 4))\n",
    "for i, comp in enumerate(rbm.components_):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.suptitle('100 components extracted by RBM', fontsize=16)\n",
    "plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
